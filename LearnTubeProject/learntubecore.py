# -*- coding: utf-8 -*-
"""LearnTubeCore.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12cRHAZ-ACGdSxNMaeEsmRf7FOT27ayN6

Steps:

* Extract audio from the videos.
* Transcribe the audio to text.
* Create and save embeddings generated from transcriptions (Vector DB).
* Retrieve similar text from the Vector DB.
* Utilize the similar content as context for Large Language Model (LLM) response generation.

#<p style="background-color:#682F2F;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:10px 10px;">Libraries</p>
"""

#Youtube LIB
from yt_dlp import YoutubeDL

#DL framaeworks
import torch

#File Systems
import os
import glob
import pathlib
import shutil

#utils
from IPython.display import Audio
from tqdm.notebook import tqdm

#Doument Spliter Langchain
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain.document_loaders import JSONLoader,DirectoryLoader,TextLoader
from langchain.document_loaders import AssemblyAIAudioTranscriptLoader


# vector stores
import openai

#Langchain
from langchain.chains.summarize import load_summarize_chain
from langchain import PromptTemplate, LLMChain
from langchain.chains import RetrievalQA
from langchain.schema import HumanMessage
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain.schema.runnable import RunnablePassthrough
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Pinecone
from langchain.embeddings import OpenAIEmbeddings
from faster_whisper import WhisperModel
from langchain_pinecone import PineconeVectorStore
import tiktoken

"""setup the kyes"""

import os
from dotenv import load_dotenv

# Load API keys from .env file
load_dotenv()

# Configuration
openai_api_key = os.getenv("OPENAI_API_KEY")
pinecone_api_key = os.getenv("PINECONE_API_KEY")
pinecone_env = os.getenv("PINECONE_ENV")  # e.g., "us-west1-gcp"
pinecone_index_name = "my-index"  # choose an appropriate name

# LangSmith API Key & Tracing
langsmith_api_key = os.getenv("LANGCHAIN_API_KEY")
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_API_KEY"] = langsmith_api_key
# Print validation of .env values
print("‚úÖ OPENAI_API_KEY loaded:", bool(openai_api_key))
print("‚úÖ PINECONE_API_KEY loaded:", bool(pinecone_api_key))
print("‚úÖ PINECONE_ENV loaded:", bool(pinecone_env))
print("‚úÖ LANGCHAIN_API_KEY loaded:", bool(langsmith_api_key))

"""cofigration"""

class CFG:
    # Language Model Configuration (OpenAI GPT)
    model_name = 'gpt-4'
    temperature = 0.01
    max_tokens  = 1024

    # Audio Transcription Model (Faster-Whisper)
    transcribe_model = 'small'  # Options: 'tiny', 'base', 'small', 'medium', 'large-v2'
    save_transcriptions = True
    transcribe_files = False

    # YouTube Playlist or Audio Source
    playlist_links = [
        'https://www.youtube.com/playlist?list=PLdKd-j64gDcDVXmhHLIRIqpfnxiJadMjd',

    ]

    audio_inputs_path = '/content/audio_inputs/'

    # Path to save transcripted text files
    save_transcript_path = '/content/transcriptions/'

    # Transcription parameters (optional placeholders)
    chunk_length = 10
    batch_size = 4

    # Embeddings Configuration (OpenAI + Pinecone)
    embedding_provider = 'openai'
    openai_api_key = os.getenv("OPENAI_API_KEY")
    openai_embedding_dim = 1536
    doc_similarity = 3  # Number of top similar documents to retrieve

    # Document Chunking for Retrieval
    chunk_size = 1000
    chunk_overlap = 0

"""# <p style="background-color:#682F2F;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:10px 10px;">üé• Video to Audio Extraction</p>"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Download audio from YouTube
# import pathlib
# from yt_dlp import YoutubeDL
# 
# path = pathlib.Path("audio_inputs")
# path.mkdir(exist_ok=True)
# 
# ydl_opts = {
#     'format': '140',  # Audio-only format
#     'paths': {'home': path.as_posix()},
#     'outtmpl': '/content/audio_inputs/audio_%(playlist_index)s.%(ext)s'
# }
# 
# with YoutubeDL(params=ydl_opts) as ydl:
#     for playlist_link in CFG.playlist_links:
#         ydl.download(playlist_link)
# 
# # List downloaded audio files in correct playlist order
# num_files = len(CFG.playlist_links)
# audio_paths = [f'/content/audio_inputs/audio_{i+1}.m4a' for i in range(num_files)]
# print(audio_paths)
#

"""**"""

# Create directory if it doesn't exist
def create_repository(path):
    """
    Creates a repository directory if it doesn't exist.

    Args:
        path: The path to the repository directory.
    """
    try:
        os.makedirs(path)
    except:
        print('Path already exists')

create_repository(CFG.save_transcript_path)

"""# <p style="background-color:#682F2F;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:10px 10px;">üó£Ô∏è Speech to Text with Faster-Whisper
</p>



"""

# Load and initialize the Faster-Whisper model for audio transcription.
# This implementation is optimized for speed and resource efficiency,
# especially useful when working with limited hardware (e.g., CPUs or small GPUs).
# The transcriber will automatically choose the best available device (GPU or CPU),
# and it supports batching and chunking for longer audio inputs.

def load_faster_whisper_model(model_name="large-v2"):
    try:
        # Automatically selects 'cuda' if available, otherwise 'cpu'
        compute_type = "float16" if torch.cuda.is_available() else "int8"
        model = WhisperModel(
            model_size_or_path=model_name,
            device="cuda" if torch.cuda.is_available() else "cpu",
            compute_type=compute_type
        )
        print("‚úÖ Faster-Whisper model loaded successfully.")
        return model
    except Exception as e:
        print(f"‚ùå Failed to load Faster-Whisper model: {e}")
        return None

def transcribe_and_save_faster_whisper(audio_paths, save_path, model):
    os.makedirs(save_path, exist_ok=True)

    for audio_path in tqdm(audio_paths, desc="üîä Transcribing audio"):
        try:
            segments, info = model.transcribe(
                audio_path,
                beam_size=5,
                vad_filter=True,
                vad_parameters=dict(min_silence_duration_ms=500)
            )

            # Collect full transcription text
            text = ''.join([segment.text for segment in segments])

            video_title = os.path.basename(audio_path).replace(".m4a", "").replace(".mp3", "")
            output_file = os.path.join(save_path, f"{video_title}.txt")

            with open(output_file, "w", encoding="utf-8") as f:
                f.write(text)

            print(f"‚úÖ Transcription saved: {output_file}")

        except Exception as e:
            print(f"‚ùå Error processing {audio_path}: {e}")

import re
import glob

# Get all files in the folder and sort them by number
def extract_number(path):
    match = re.search(r"audio_(\d+)\.m4a", path)
    return int(match.group(1)) if match else float('inf')

audio_paths = sorted(glob.glob('/content/audio_inputs/audio_*.m4a'), key=extract_number)

print("üßæ Files to be transcribed:")
print(audio_paths)

# Load the model
faster_model = load_faster_whisper_model("small")

# Transcribe if enabled
if CFG.save_transcriptions and faster_model:
    transcribe_and_save_faster_whisper(
        audio_paths=audio_paths,
        save_path=CFG.save_transcript_path,
        model=faster_model
)

"""# <p style="background-color:#682F2F;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:10px 10px;">Documents Loader</p>"""

from google.colab import drive
drive.mount('/content/drive')

loader = DirectoryLoader(
    path = CFG.save_transcript_path,
    glob="./*.txt",
    loader_cls=TextLoader,
    show_progress=True,
    use_multithreading=False
)

docs = loader.load()
print('Documents: ',len(docs))

docs[0]

"""## Audio vs Transcription"""

from IPython.display import Audio

for i, audio_file in enumerate(audio_paths):
    print(f'Audio {i + 1}')
    display(Audio(audio_file, autoplay=False))

# Extract transcribed texts from the first 9 audio files
for i in range(9):
    audio_name = audio_paths[i].split('/')[-1][:-4]
    transcription = [
        doc.page_content
        for doc in docs
        if doc.metadata['source'].split('/')[-1][:-4] == audio_name
    ]

    print(f"\nüìÑ Transcription for {audio_name}:")
    if transcription:
        print(transcription[0][:500])  # Print the first 500 characters
    else:
        print("‚ùå No transcription found for this audio file.")

"""# <p style="background-color:#682F2F;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:10px 10px;">üß† Text Chunking and Embedding</p>

Divide the text into chunks, the search works on this chunks to perform the retrieval.
"""

from langchain.text_splitter import RecursiveCharacterTextSplitter

# Split documents into manageable chunks
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=300,
    chunk_overlap=50
)

# Assuming `docs` is a list of LangChain Document objects
chunks = text_splitter.split_documents(docs)
print(f"Number of chunks: {len(chunks)}")

import pinecone
import os

# Initialize Pinecone client
pc = pinecone.Pinecone(api_key=os.getenv("PINECONE_API_KEY"))

# List current indexes
print("Current indexes:", pc.list_indexes().names())

# Delete a specific index by name
index_name = "my-index"  # ‚Üê Replace with your index name
if index_name in pc.list_indexes().names():
    pc.delete_index(index_name)
    print(f"Index '{index_name}' deleted.")
else:
    print(f"Index '{index_name}' does not exist.")

import pinecone
from pinecone import ServerlessSpec
import os

# Initialize Pinecone
pc = pinecone.Pinecone(api_key=os.getenv("PINECONE_API_KEY")) # Initialize pinecone first
index_name = "my-index"

pc.create_index(
            name=index_name,
            dimension=1536,  # OpenAI embeddings dimension
            metric="cosine",
            spec=ServerlessSpec(cloud="aws", region="us-east-1")
        )

embeddings = OpenAIEmbeddings(openai_api_key=os.getenv("OPENAI_API_KEY"))
# Connect to the index
index = pc.Index(index_name)

# Upsert the chunks into Pinecone
batch_size = 32  # Adjust based on your index and data
for i in tqdm(range(0, len(chunks), batch_size), desc="Upserting chunks"):
    i_end = min(i + batch_size, len(chunks))
    chunk_batch = chunks[i:i_end]
    ids_batch = [str(n) for n in range(i, i_end)]
    texts = [x.page_content for x in chunk_batch]
    embeds = embeddings.embed_documents(texts)
    metadata = [{'text': x.page_content, 'source': x.metadata['source']} for x in chunk_batch]
    index.upsert(vectors=zip(ids_batch, embeds, metadata))

text = "This is a sample chunk."
embedding_vector = embeddings.embed_query(text)
print(len(embedding_vector))
print(embedding_vector[:5])

"""# <p style="background-color:#682F2F;font-family:newtimeroman;color:#FFF9ED;font-size:150%;text-align:center;border-radius:10px 10px;">üîç Vector Store with Pinecone</p>"""

# langchain wrapper around Pinecone
vectorstore = PineconeVectorStore(
    pinecone_api_key=pinecone_api_key,
    index_name=index_name,
    embedding=embeddings,
    text_key="text",  # Indicating in which field the actual text is located
)
vectorstore.as_retriever().invoke("Advantages ")

"""# **RAG**

üí¨ Natural Language QA with LangChain
"""

# RAG Chain
llm = OpenAI()
qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=vectorstore.as_retriever())

query = "What is Generative AI?"
response = qa_chain.run(query)
print("Answer:", response)